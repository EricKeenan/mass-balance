{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a training dataset for ML-based $\\Delta$FAC from multiple SNOWPACK simulations\n",
    "The training dataset needs columns of the following (where each row represents one example):\n",
    "1. 91 day $\\Delta$FAC (m)\n",
    "2. 91 day average precipitation (mm/day)\n",
    "3. 1980 - 2021 precipitation climatology (mm/day)\n",
    "4. 91 day average temperature (C)\n",
    "5. 1980 - 2021 temperature climatology (C)\n",
    "6. (10 day precip before tf) - (10 day precip before t0) (mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First, get a list of SNOWPACK sites that have completed their simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd /scratch/summit/erke2265/mass-balance/100_sites/output/\n",
    "ls -- *.sno | wc -l\n",
    "basename -s .sno -- *.sno > /scratch/summit/erke2265/mass-balance/notebooks/finished.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next, load an array of strings composed of the finished sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['-66.500_103.750_SPINUP', '-67.500_123.125_SPINUP'], dtype='<U23')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sites = np.loadtxt(\"finished.txt\", dtype=str)[0:2]\n",
    "sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now get training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_smet(path, var):\n",
    "\n",
    "    \"\"\" Reads a .smet file and returns a time series of the defined variable as a pandas data frame.\n",
    "    Args:\n",
    "        path (str): String pointing to the location of the .smet file to be read.\n",
    "        var  (str): Variable you want to plot\n",
    "    Returns:\n",
    "        Time series of defined variable as a pandas data frame.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load .smet file as a Pandas data frame\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # Determine indices for data retrieval\n",
    "    bump = 2\n",
    "\n",
    "    fields_row = np.where(df[df.columns[0]].str.startswith(\"fields\"))[0][0] + bump\n",
    "    data_row = np.where(df[df.columns[0]] == '[DATA]')[0][0] + bump\n",
    "    fields =  np.loadtxt(path, skiprows=fields_row - 1, max_rows=1, dtype='str')\n",
    "    data_col = []\n",
    "    for k in range(0, len(var)):\n",
    "        for j in range(0, len(list(fields[2:]))):\n",
    "            if var[k] == fields[2+j]:\n",
    "                data_col.append(j)\n",
    "\n",
    "    # Creates pandas data frame\n",
    "    time = np.loadtxt(path, skiprows=data_row, usecols=0, dtype = 'str')\n",
    "    time = pd.to_datetime(time, format='%Y-%m-%dT%H:%M:%S')\n",
    "    data = np.loadtxt(path, skiprows=data_row, usecols=tuple(data_col))\n",
    "    df = pd.DataFrame(data, index=time)\n",
    "\n",
    "    # Set no data values to nan\n",
    "    df[df == -999] = np.nan\n",
    "\n",
    "    # Return time series as daily average Pandas data frame\n",
    "    return df\n",
    "\n",
    "def calc_FAC(HS, SWE):\n",
    "    \n",
    "    \"\"\"\n",
    "    SNOWPACK assumes the density of ice is 917 kg/m^3\n",
    "    \"\"\"\n",
    "    \n",
    "    SWE_in_units_ice_equivalent = SWE / 0.917\n",
    "    FAC = HS - SWE_in_units_ice_equivalent\n",
    "    return FAC\n",
    "\n",
    "def calc_training_data(site):\n",
    "    \n",
    "    # File paths\n",
    "    meteo_smet = \"/scratch/summit/erke2265/mass-balance/100_sites/smet/\" + site[:-7]+ \".smet\"\n",
    "    snowpack_smet = \"/scratch/summit/erke2265/mass-balance/100_sites/output/\" + site[:]+ \".smet\"\n",
    "    \n",
    "    # Meteo data\n",
    "    meteo_df = read_smet(meteo_smet, [\"TA\", \"U\", \"V\", \"PSUM\"])\n",
    "    snowpack_df = read_smet(snowpack_smet, [\"HS_mod\", \"SWE\"])\n",
    "    \n",
    "    # Temperature\n",
    "    ta = meteo_df[0].resample('D').mean()\n",
    "    ta_mean = ta.mean() # Use in traning\n",
    "    ta_91_day_mean = ta.rolling(91).mean() # Use in traning\n",
    "\n",
    "    # Precip\n",
    "    psum = meteo_df[3].resample('D').mean()\n",
    "    psum_mean = psum.mean() # Use in traning\n",
    "    psum_91_day_mean = psum.rolling(91).mean() # Use in traning\n",
    "    \n",
    "    # 10 day precip\n",
    "    precip_10_day_mean = psum.rolling(10).mean()\n",
    "    precip_10_day_mean_diff = precip_10_day_mean.diff(periods=91) # Use in traning\n",
    "    \n",
    "    # Change in FAC\n",
    "    HS = snowpack_df[0] / 100 # Units: m\n",
    "    SWE = snowpack_df[1] / 1000 # Units: m\n",
    "\n",
    "    HS = HS.resample('D').mean()\n",
    "    SWE = SWE.resample('D').mean()\n",
    "\n",
    "    FAC = calc_FAC(HS, SWE)\n",
    "    delta_FAC_91_day = FAC.diff(periods=91) # Use in traning\n",
    "    \n",
    "    # Wind Speed\n",
    "    u = meteo_df[1].resample('D').mean()\n",
    "    u_mean = u.mean()\n",
    "    v = meteo_df[2].resample('D').mean()\n",
    "    v_mean = v.mean()\n",
    "    ws_mean = np.sqrt(u_mean**2 + v_mean**2) # Use in traning\n",
    "    \n",
    "    # Return data\n",
    "    return ta_mean, ta_91_day_mean, psum_mean, psum_91_day_mean, precip_10_day_mean_diff, delta_FAC_91_day, ws_mean\n",
    "\n",
    "def read_smet_lat_lon_ratio(path):\n",
    "\n",
    "    \"\"\" Reads a .smet file and returns a time series of the defined variable as a pandas data frame.\n",
    "    Args:\n",
    "        path (str): String pointing to the location of the .smet file to be read.\n",
    "        var  (str): Variable you want to plot\n",
    "    Returns:\n",
    "        Time series of defined variable as a pandas data frame.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load .smet file as a Pandas data frame\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    lat_row = np.where(df[df.columns[0]].str.startswith(\"latitude\"))[0][0]\n",
    "    lon_row = np.where(df[df.columns[0]].str.startswith(\"longitude\"))[0][0]\n",
    "    ratio_row = np.where(df[df.columns[0]].str.startswith(\"units_multiplier\"))[0][0]\n",
    "    \n",
    "    lat = float(df['SMET 1.1 ASCII'][lat_row].split()[-1])\n",
    "    lon = float(df['SMET 1.1 ASCII'][lon_row].split()[-1])\n",
    "    ratio = float(df['SMET 1.1 ASCII'][ratio_row].split()[-2])\n",
    "    if np.isnan(ratio):\n",
    "        ratio = 1\n",
    "\n",
    "    return lat, lon, ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Initialize training array\n",
    "train = np.array([])\n",
    "\n",
    "# Sampling index (hacky... sorry)\n",
    "ind_1980 = -365 * 42 + 89 # After using x5\n",
    "\n",
    "# Loop over each site and append training data\n",
    "for j in range(0, len(sites)):\n",
    "    print(j)\n",
    "    \n",
    "    # Get data\n",
    "    ta_mean, ta_91_day_mean, psum_mean, psum_91_day_mean, precip_10_day_mean_diff, delta_FAC_91_day, ws_mean = calc_training_data(sites[j])\n",
    "    \n",
    "    # Get sin and cos of day of year (cyclic feature)\n",
    "    doy = np.zeros(len(ta_91_day_mean.index)); doy[:] = np.nan # day of year\n",
    "    for k in range(0, len(ta_91_day_mean.index)):\n",
    "        doy[k] = ta_91_day_mean.index[k].timetuple().tm_yday\n",
    "    day_sin = np.sin(doy*(2.*np.pi/365))\n",
    "    day_cos = np.cos(doy*(2.*np.pi/365))\n",
    "    \n",
    "    # Get lat, lon, and ratio\n",
    "    smet_path = \"/scratch/summit/erke2265/mass-balance/100_sites/smet/\" + sites[j][:-7]+ \".smet\"\n",
    "    lat, lon, ratio = read_smet_lat_lon_ratio(smet_path)\n",
    "    \n",
    "    # Target data\n",
    "    Y = np.squeeze(delta_FAC_91_day[ind_1980:].values)\n",
    "\n",
    "    # Features\n",
    "    x1 = np.squeeze(psum_91_day_mean[ind_1980:].values)\n",
    "    x2 = np.repeat(psum_mean, len(x1))\n",
    "    x3 = np.squeeze(ta_91_day_mean[ind_1980:].values)\n",
    "    x4 = np.repeat(ta_mean, len(x1))\n",
    "    x5 = np.squeeze(precip_10_day_mean_diff[ind_1980:].values)\n",
    "    x6 = np.repeat(ratio, len(x1))\n",
    "    x7 = day_sin[ind_1980:]\n",
    "    x8 = day_cos[ind_1980:]\n",
    "    x9 = np.repeat(ws_mean, len(x1))\n",
    "    \n",
    "    # Combine features into readibly savable format\n",
    "    rand_ind = np.random.choice(np.arange(0, len(x1)), int(len(x1) / 10), replace=False)\n",
    "    site_lat_lon = np.array([[lat, lon]])\n",
    "    site_train = np.transpose(np.stack([Y, x1, x2, x3, x4, x5, x6, x7, x8, x9]))\n",
    "    site_train = site_train[rand_ind, :]\n",
    "    \n",
    "    # Single array\n",
    "    if j == 0: # First site \n",
    "        lat_lon = site_lat_lon\n",
    "        train = site_train\n",
    "    else: # All other sites\n",
    "        lat_lon = np.concatenate([lat_lon, site_lat_lon])\n",
    "        train = np.concatenate([train, site_train])\n",
    "        \n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle and save training array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(train)\n",
    "print(train.shape)\n",
    "np.savetxt(\"train.txt\", train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get coastlines\n",
    "df = gpd.read_file(\"/pl/active/nasa_smb/Data/ADD_Coastline_low_res_polygon.shp\")\n",
    "crs_epsg = ccrs.SouthPolarStereo()\n",
    "df_epsg = df.to_crs(epsg='3031')\n",
    "\n",
    "# Generate figure \n",
    "fig, axs = plt.subplots(1, 1, subplot_kw={'projection': crs_epsg},\n",
    "                        figsize=(15, 15))\n",
    "\n",
    "# Plot sites\n",
    "plt.scatter(lat_lon[:,1], lat_lon[:,0], linewidth=3, transform=ccrs.PlateCarree())\n",
    "\n",
    "\n",
    "# Plot coastlines\n",
    "axs.set_extent((-180, 180, -90, -65), ccrs.PlateCarree())\n",
    "axs.add_geometries(df_epsg['geometry'], crs=crs_epsg,\n",
    "                      facecolor='none', edgecolor='black')\n",
    "print(lat_lon.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alpine3d",
   "language": "python",
   "name": "alpine3d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
