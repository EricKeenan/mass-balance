{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a training dataset for ML-based $\\Delta$FAC from multiple SNOWPACK simulations\n",
    "The training dataset needs columns of the following (where each row represents one example):\n",
    "1. 91 day $\\Delta$FAC (m)\n",
    "2. 1980 - 2021 temperature climatology (C)\n",
    "3. 91 day average temperature (C)\n",
    "4. 1980 - 2021 precipitation climatology (mm/s)\n",
    "5. 91 day average precipitation (mm/s)\n",
    "6. 91 - 180 day average precipitation (mm/s)\n",
    "7. 180 - 365 day average precipitation (mm/s)\n",
    "8. 365 - 730 day average precipitation (mm/s)\n",
    "9. Date sine\n",
    "10. Date cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import geopandas as gpd\n",
    "import glob\n",
    "from joblib import Parallel, delayed, memory\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First, get a list of SNOWPACK sites that have completed their simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd /scratch/summit/erke2265/mass-balance/400_sites/output/\n",
    "ls -- *.sno | wc -l\n",
    "basename -s .sno -- *.sno > /scratch/summit/erke2265/mass-balance/notebooks/finished.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next, load an array of strings composed of the finished sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396\n"
     ]
    }
   ],
   "source": [
    "sites = np.loadtxt(\"finished.txt\", dtype=str)\n",
    "print(len(sites))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ratio(path):\n",
    "    \n",
    "    # Load .smet file as a Pandas data frame\n",
    "    df = pd.read_csv(path)\n",
    "    \n",
    "    # Get ratio\n",
    "    ratio_row = np.where(df[df.columns[0]].str.startswith(\"units_multiplier\"))[0][0]\n",
    "    ratio = float(df['SMET 1.1 ASCII'][ratio_row].split()[-2])\n",
    "    if np.isnan(ratio):\n",
    "        ratio = 1\n",
    "        \n",
    "    return ratio\n",
    "    \n",
    "def read_smet(path, var):\n",
    "\n",
    "    \"\"\" Reads a .smet file and returns a time series of the defined variable as a pandas data frame.\n",
    "    Args:\n",
    "        path (str): String pointing to the location of the .smet file to be read.\n",
    "        var  (str): Variable you want to plot\n",
    "    Returns:\n",
    "        Time series of defined variable as a pandas data frame.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load .smet file as a Pandas data frame\n",
    "    df = pd.read_csv(path)\n",
    "    \n",
    "    # Get lat, lon, ratio\n",
    "    lat_row = np.where(df[df.columns[0]].str.startswith(\"latitude\"))[0][0]\n",
    "    lon_row = np.where(df[df.columns[0]].str.startswith(\"longitude\"))[0][0]\n",
    "    \n",
    "    lat = float(df['SMET 1.1 ASCII'][lat_row].split()[-1])\n",
    "    lon = float(df['SMET 1.1 ASCII'][lon_row].split()[-1])\n",
    "\n",
    "    # Determine indices for data retrieval\n",
    "    bump = 2\n",
    "\n",
    "    fields_row = np.where(df[df.columns[0]].str.startswith(\"fields\"))[0][0] + bump\n",
    "    data_row = np.where(df[df.columns[0]] == '[DATA]')[0][0] + bump\n",
    "    fields =  np.loadtxt(path, skiprows=fields_row - 1, max_rows=1, dtype='str')\n",
    "    data_col = []\n",
    "    for k in range(0, len(var)):\n",
    "        for j in range(0, len(list(fields[2:]))):\n",
    "            if var[k] == fields[2+j]:\n",
    "                data_col.append(j)\n",
    "\n",
    "    # Creates pandas data frame\n",
    "    time = np.loadtxt(path, skiprows=data_row, usecols=0, dtype = 'str')\n",
    "    time = pd.to_datetime(time, format='%Y-%m-%dT%H:%M:%S')\n",
    "    data = np.loadtxt(path, skiprows=data_row, usecols=tuple(data_col))\n",
    "    df = pd.DataFrame(data, index=time)\n",
    "\n",
    "    # Set no data values to nan\n",
    "    df[df == -999] = np.nan\n",
    "\n",
    "    # Return time series as daily average Pandas data frame\n",
    "    return df, lat, lon\n",
    "\n",
    "def calc_FAC(HS, SWE):\n",
    "    \n",
    "    \"\"\"\n",
    "    SNOWPACK assumes the density of ice is 917 kg/m^3\n",
    "    \"\"\"\n",
    "    \n",
    "    SWE_in_units_ice_equivalent = SWE / 0.917\n",
    "    FAC = HS - SWE_in_units_ice_equivalent\n",
    "    return FAC\n",
    "\n",
    "def special_rolling_mean(df, n_min, n_max):\n",
    "    special_mean = ((n_max * df.rolling(n_max).mean()) - (n_min * df.rolling(n_min).mean())) / (n_max - n_min)\n",
    "    return special_mean\n",
    "\n",
    "def calc_training_data(site):\n",
    "    \n",
    "    # File paths\n",
    "    snowpack_smet = \"/scratch/summit/erke2265/mass-balance/400_sites/output/\" + site[:]+ \".smet\"\n",
    "    meteo_smet = \"/scratch/summit/erke2265/mass-balance/400_sites/smet/\" + site.replace(\"_SPINUP\", \"\", 1) + \".smet\"\n",
    "    \n",
    "    # Meteo data\n",
    "    snowpack_df, lat, lon = read_smet(snowpack_smet, [\"HS_mod\", \"SWE\"])\n",
    "    ratio = read_ratio(meteo_smet)\n",
    "    \n",
    "    # Temperature\n",
    "    M2_T2M = xr.open_mfdataset(\"M2_T2M_1980-2021.nc\", combine='nested', concat_dim='time')\n",
    "    ta_mean = M2_T2M.sel(lat=lat, lon=lon, method='nearest')['T2M'].mean(dim='time').values # Units: C\n",
    "    ta_91_day_mean = M2_T2M.sel(lat=lat, lon=lon, method='nearest')['T2M'].rolling(time=91).mean().values # Units: C\n",
    "    M2_T2M.close()\n",
    "\n",
    "    # Precip\n",
    "    M2_PSUM = xr.open_mfdataset(\"M2_PSUM_1980-2021.nc\", combine='nested', concat_dim='time')\n",
    "    psum_mean = M2_PSUM.sel(lat=lat, lon=lon, method='nearest')['__xarray_dataarray_variable__'].mean(dim='time').values # Units: mm/hr\n",
    "    psum_91_day_mean = M2_PSUM.sel(lat=lat, lon=lon, method='nearest')['__xarray_dataarray_variable__'].rolling(time=91).mean().values # Units: mm/hr\n",
    "    psum_180_day_mean = M2_PSUM.sel(lat=lat, lon=lon, method='nearest')['__xarray_dataarray_variable__'].rolling(time=180).mean().values # Units: mm/hr\n",
    "    psum_365_day_mean = M2_PSUM.sel(lat=lat, lon=lon, method='nearest')['__xarray_dataarray_variable__'].rolling(time=365).mean().values # Units: mm/hr\n",
    "    psum_730_day_mean = M2_PSUM.sel(lat=lat, lon=lon, method='nearest')['__xarray_dataarray_variable__'].rolling(time=730).mean().values # Units: mm/hr\n",
    "    M2_PSUM.close()\n",
    "    rolling_psum_180_day_mean = (180 * psum_180_day_mean - 91 * psum_91_day_mean) / (180 - 91)\n",
    "    rolling_psum_365_day_mean = (365 * psum_365_day_mean - 180 * psum_180_day_mean) / (365 - 180)\n",
    "    rolling_psum_730_day_mean = (730 * psum_730_day_mean - 365 * psum_365_day_mean) / (730 - 365)\n",
    "    \n",
    "    # Change in FAC\n",
    "    HS = snowpack_df[0] / 100 # Units: m\n",
    "    SWE = snowpack_df[1] / 1000 # Units: m\n",
    "\n",
    "    HS = HS.resample('D').mean()\n",
    "    SWE = SWE.resample('D').mean()\n",
    "\n",
    "    FAC = calc_FAC(HS, SWE)\n",
    "    delta_FAC_91_day = FAC.diff(periods=91) # Use in traning\n",
    "    \n",
    "    # Return data\n",
    "    return ta_mean, ta_91_day_mean, psum_mean, psum_91_day_mean, rolling_psum_180_day_mean, rolling_psum_365_day_mean, rolling_psum_730_day_mean, delta_FAC_91_day, lat, lon, ratio\n",
    "\n",
    "\n",
    "def save_site(site, index):\n",
    "    \n",
    "    # Initialize training array\n",
    "    train = np.array([])\n",
    "    \n",
    "    # Sampling index (hacky... sorry)\n",
    "    ind_1980 = 729 # 2 year\n",
    "    \n",
    "    # Get data\n",
    "    ta_mean, ta_91_day_mean, psum_mean, psum_91_day_mean, psum_180_day_mean, psum_365_day_mean, psum_730_day_mean, delta_FAC_91_day, lat, lon, ratio = calc_training_data(site)\n",
    "\n",
    "\n",
    "    # Get sin and cos of day of year (cyclic feature)\n",
    "    doy = np.zeros(len(delta_FAC_91_day.index)); doy[:] = np.nan # day of year\n",
    "    for k in range(0, len(delta_FAC_91_day.index)):\n",
    "        doy[k] = delta_FAC_91_day.index[k].timetuple().tm_yday\n",
    "    day_sin = np.sin(doy*(2.*np.pi/365))\n",
    "    day_cos = np.cos(doy*(2.*np.pi/365))\n",
    "    \n",
    "    # Target data\n",
    "    Y = np.squeeze(delta_FAC_91_day[ind_1980:].values)\n",
    "\n",
    "    # Features\n",
    "    RF_SMB_perturbation = np.repeat(ratio, len(Y))\n",
    "    x1 = np.repeat(ta_mean, len(Y))\n",
    "    x2 = np.squeeze(ta_91_day_mean[ind_1980:])\n",
    "    x3 = np.repeat(psum_mean, len(Y)) * RF_SMB_perturbation\n",
    "    x4 = np.squeeze(psum_91_day_mean[ind_1980:]) * RF_SMB_perturbation\n",
    "    x5 = np.squeeze(psum_180_day_mean[ind_1980:]) * RF_SMB_perturbation\n",
    "    x6 = np.squeeze(psum_365_day_mean[ind_1980:]) * RF_SMB_perturbation\n",
    "    x7 = np.squeeze(psum_730_day_mean[ind_1980:]) * RF_SMB_perturbation\n",
    "    x8 = day_sin[ind_1980:]\n",
    "    x9 = day_cos[ind_1980:]\n",
    "\n",
    "    # Combine features into readibly savable format\n",
    "    lat_lon = np.array([[lat, lon]])\n",
    "    train = np.transpose(np.stack([Y, x1, x2, x3, x4, x5, x6, x7, x8, x9]))\n",
    "    \n",
    "    # Save array\n",
    "    np.save(\"training_data/\" + str(index) + \"_train.npy\", train)\n",
    "    np.save(\"training_data/\" + str(index) + \"_lat_lon.npy\", lat_lon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop over each site and save array of training data and lat/lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "!rm training_data/*\n",
    "# Parallel(n_jobs=12)(delayed(save_site)(sites[j], j) for j in range(0, len(sites)))\n",
    "for j in range(0, 1):\n",
    "    print(j)\n",
    "    save_site(sites[j], j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filenames = glob.glob(\"training_data/*lat_lon.npy\")\n",
    "# lat_lon = np.array([np.load(fname) for fname in filenames])\n",
    "# lat_lon = lat_lon.reshape(lat_lon.shape[1]*lat_lon.shape[0], lat_lon.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get coastlines\n",
    "# df = gpd.read_file(\"/pl/active/nasa_smb/Data/ADD_Coastline_low_res_polygon.shp\")\n",
    "# crs_epsg = ccrs.SouthPolarStereo()\n",
    "# df_epsg = df.to_crs(epsg='3031')\n",
    "\n",
    "# # Generate figure \n",
    "# fig, axs = plt.subplots(1, 1, subplot_kw={'projection': crs_epsg},\n",
    "#                         figsize=(15, 15))\n",
    "\n",
    "# # Plot sites\n",
    "# plt.scatter(lat_lon[:,1], lat_lon[:,0], linewidth=3, transform=ccrs.PlateCarree())\n",
    "\n",
    "\n",
    "# # Plot coastlines\n",
    "# axs.set_extent((-180, 180, -90, -65), ccrs.PlateCarree())\n",
    "# axs.add_geometries(df_epsg['geometry'], crs=crs_epsg,\n",
    "#                       facecolor='none', edgecolor='black')\n",
    "# print(lat_lon.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alpine3d",
   "language": "python",
   "name": "alpine3d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
